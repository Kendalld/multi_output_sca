{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-Output MLP for Side-Channel Analysis\n",
        "\n",
        "This notebook implements the MLP_MO (Multi-Output Multilayer Perceptron) architecture for non-profiled side-channel attack on AES-128.\n",
        "\n",
        "**Goal:** Recover the AES-128 secret key from power traces by training a single neural network to predict all 256 possible key bytes simultaneously.\n",
        "\n",
        "**Memory Constraint:** Max 25GB RAM - uses lazy loading from HDF5 file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Block 1: Setup & Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Batch size: 1000\n",
            "Learning rate: 0.001\n",
            "Number of epochs: 30\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Constants\n",
        "BATCH_SIZE = 1000\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_EPOCHS = 30\n",
        "NUM_BRANCHES = 256  # One branch for each key hypothesis (0-255)\n",
        "TRACE_LENGTH = 700\n",
        "DATASET_PATH = Path('dataset/ascadv2-extracted.h5')\n",
        "\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"Number of epochs: {NUM_EPOCHS}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Block 2: Verify H5 File Structure\n",
        "\n",
        "First, let's inspect the HDF5 file to understand its structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-level keys: ['Attack_traces', 'Profiling_traces']\n"
          ]
        }
      ],
      "source": [
        "# Verify H5 file structure\n",
        "if DATASET_PATH.exists():\n",
        "    with h5py.File(DATASET_PATH, 'r') as f:\n",
        "        print(\"Top-level keys:\", list(f.keys()))\n",
        "        if 'traces' in f:\n",
        "            print(f\"Traces shape: {f['traces'].shape}\")\n",
        "            print(f\"Traces dtype: {f['traces'].dtype}\")\n",
        "        if 'metadata' in f:\n",
        "            print(\"Metadata keys:\", list(f['metadata'].keys()))\n",
        "            if 'plaintext' in f['metadata']:\n",
        "                print(f\"Plaintext shape: {f['metadata']['plaintext'].shape}\")\n",
        "                print(f\"Plaintext dtype: {f['metadata']['plaintext'].dtype}\")\n",
        "            if 'key' in f['metadata']:\n",
        "                print(f\"Key shape: {f['metadata']['key'].shape}\")\n",
        "                print(f\"Key value: {f['metadata']['key'][:]}\")\n",
        "        # Check for alternative key names\n",
        "        if 'inputs' in f:\n",
        "            print(\"Found 'inputs' key\")\n",
        "            print(\"Inputs keys:\", list(f['inputs'].keys()) if hasattr(f['inputs'], 'keys') else \"Not a group\")\n",
        "else:\n",
        "    print(f\"Warning: Dataset file not found at {DATASET_PATH}\")\n",
        "    print(\"Please ensure the HDF5 file is placed in the dataset/ directory\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Block 3: Memory-Efficient Data Loader\n",
        "\n",
        "Implement a custom Dataset class that uses lazy loading to avoid loading the entire dataset into memory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error creating dataset: \"Could not find traces. Available keys: ['Attack_traces', 'Profiling_traces']\"\n",
            "You may need to adjust the key names based on your HDF5 file structure\n"
          ]
        }
      ],
      "source": [
        "class ASCADDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Memory-efficient dataset for ASCAD power traces.\n",
        "    Uses lazy loading - only reads data from disk when requested.\n",
        "    \"\"\"\n",
        "    def __init__(self, h5_path, trace_key='traces', plaintext_key='metadata/plaintext'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            h5_path: Path to the HDF5 file\n",
        "            trace_key: Key for traces in HDF5 file (default: 'traces')\n",
        "            plaintext_key: Key for plaintexts in HDF5 file (default: 'metadata/plaintext')\n",
        "        \"\"\"\n",
        "        self.h5_path = h5_path\n",
        "        self.trace_key = trace_key\n",
        "        self.plaintext_key = plaintext_key\n",
        "        \n",
        "        # Open file to get length (but keep it open for lazy loading)\n",
        "        with h5py.File(h5_path, 'r') as f:\n",
        "            # Try different possible key structures\n",
        "            if trace_key in f:\n",
        "                self.length = len(f[trace_key])\n",
        "            elif 'inputs' in f and 'traces' in f['inputs']:\n",
        "                self.trace_key = 'inputs/traces'\n",
        "                self.length = len(f['inputs']['traces'])\n",
        "            else:\n",
        "                raise KeyError(f\"Could not find traces. Available keys: {list(f.keys())}\")\n",
        "            \n",
        "            # Get plaintext key\n",
        "            if plaintext_key in f:\n",
        "                pass  # Key exists\n",
        "            elif 'inputs' in f and 'plaintext' in f['inputs']:\n",
        "                self.plaintext_key = 'inputs/plaintext'\n",
        "            elif 'metadata' in f and 'plaintext' in f['metadata']:\n",
        "                self.plaintext_key = 'metadata/plaintext'\n",
        "            else:\n",
        "                # Try to find any plaintext-like key\n",
        "                with h5py.File(h5_path, 'r') as check_f:\n",
        "                    def find_key(name, obj):\n",
        "                        if 'plaintext' in name.lower():\n",
        "                            print(f\"Found potential plaintext key: {name}\")\n",
        "                    check_f.visititems(find_key)\n",
        "                raise KeyError(f\"Could not find plaintext. Checked: {plaintext_key}\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Read a single trace and plaintext from disk.\n",
        "        Returns:\n",
        "            trace: torch.Tensor of shape (700,) dtype float32\n",
        "            plaintext: torch.Tensor scalar dtype long\n",
        "        \"\"\"\n",
        "        # Open file for each access (h5py handles this efficiently)\n",
        "        with h5py.File(self.h5_path, 'r') as f:\n",
        "            # Read trace - shape (700,)\n",
        "            trace = f[self.trace_key][idx]\n",
        "            # Read plaintext - scalar\n",
        "            plaintext = f[self.plaintext_key][idx]\n",
        "        \n",
        "        # Convert to torch tensors with appropriate dtypes\n",
        "        trace_tensor = torch.from_numpy(trace).float()  # (700,) float32\n",
        "        plaintext_tensor = torch.tensor(plaintext, dtype=torch.long)  # scalar long\n",
        "        \n",
        "        return trace_tensor, plaintext_tensor\n",
        "\n",
        "# Test the dataset\n",
        "if DATASET_PATH.exists():\n",
        "    try:\n",
        "        dataset = ASCADDataset(DATASET_PATH)\n",
        "        print(f\"Dataset length: {len(dataset)}\")\n",
        "        \n",
        "        # Test loading a single sample\n",
        "        trace, plaintext = dataset[0]\n",
        "        print(f\"Sample trace shape: {trace.shape}, dtype: {trace.dtype}\")\n",
        "        print(f\"Sample plaintext: {plaintext.item()}, dtype: {plaintext.dtype}\")\n",
        "        \n",
        "        # Create DataLoader\n",
        "        dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "        \n",
        "        # Test a batch\n",
        "        traces_batch, plaintexts_batch = next(iter(dataloader))\n",
        "        print(f\"\\nBatch traces shape: {traces_batch.shape}\")\n",
        "        print(f\"Batch plaintexts shape: {plaintexts_batch.shape}\")\n",
        "        print(f\"Batch traces dtype: {traces_batch.dtype}\")\n",
        "        print(f\"Batch plaintexts dtype: {plaintexts_batch.dtype}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating dataset: {e}\")\n",
        "        print(\"You may need to adjust the key names based on your HDF5 file structure\")\n",
        "else:\n",
        "    print(\"Dataset file not found. Please add the HDF5 file to continue.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Block 4: AES S-box Definition\n",
        "\n",
        "Define the AES S-box lookup table for label generation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AES S-box shape: (256,)\n",
            "AES S-box dtype: uint8\n",
            "First 16 values: [ 99 124 119 123 242 107 111 197  48   1 103  43 254 215 171 118]\n"
          ]
        }
      ],
      "source": [
        "# AES S-box (Substitution Box) - 256 byte lookup table\n",
        "AES_SBOX = np.array([\n",
        "    0x63, 0x7c, 0x77, 0x7b, 0xf2, 0x6b, 0x6f, 0xc5, 0x30, 0x01, 0x67, 0x2b, 0xfe, 0xd7, 0xab, 0x76,\n",
        "    0xca, 0x82, 0xc9, 0x7d, 0xfa, 0x59, 0x47, 0xf0, 0xad, 0xd4, 0xa2, 0xaf, 0x9c, 0xa4, 0x72, 0xc0,\n",
        "    0xb7, 0xfd, 0x93, 0x26, 0x36, 0x3f, 0xf7, 0xcc, 0x34, 0xa5, 0xe5, 0xf1, 0x71, 0xd8, 0x31, 0x15,\n",
        "    0x04, 0xc7, 0x23, 0xc3, 0x18, 0x96, 0x05, 0x9a, 0x07, 0x12, 0x80, 0xe2, 0xeb, 0x27, 0xb2, 0x75,\n",
        "    0x09, 0x83, 0x2c, 0x1a, 0x1b, 0x6e, 0x5a, 0xa0, 0x52, 0x3b, 0xd6, 0xb3, 0x29, 0xe3, 0x2f, 0x84,\n",
        "    0x53, 0xd1, 0x00, 0xed, 0x20, 0xfc, 0xb1, 0x5b, 0x6a, 0xcb, 0xbe, 0x39, 0x4a, 0x4c, 0x58, 0xcf,\n",
        "    0xd0, 0xef, 0xaa, 0xfb, 0x43, 0x4d, 0x33, 0x85, 0x45, 0xf9, 0x02, 0x7f, 0x50, 0x3c, 0x9f, 0xa8,\n",
        "    0x51, 0xa3, 0x40, 0x8f, 0x92, 0x9d, 0x38, 0xf5, 0xbc, 0xb6, 0xda, 0x21, 0x10, 0xff, 0xf3, 0xd2,\n",
        "    0xcd, 0x0c, 0x13, 0xec, 0x5f, 0x97, 0x44, 0x17, 0xc4, 0xa7, 0x7e, 0x3d, 0x64, 0x5d, 0x19, 0x73,\n",
        "    0x60, 0x81, 0x4f, 0xdc, 0x22, 0x2a, 0x90, 0x88, 0x46, 0xee, 0xb8, 0x14, 0xde, 0x5e, 0x0b, 0xdb,\n",
        "    0xe0, 0x32, 0x3a, 0x0a, 0x49, 0x06, 0x24, 0x5c, 0xc2, 0xd3, 0xac, 0x62, 0x91, 0x95, 0xe4, 0x79,\n",
        "    0xe7, 0xc8, 0x37, 0x6d, 0x8d, 0xd5, 0x4e, 0xa9, 0x6c, 0x56, 0xf4, 0xea, 0x65, 0x7a, 0xae, 0x08,\n",
        "    0xba, 0x78, 0x25, 0x2e, 0x1c, 0xa6, 0xb4, 0xc6, 0xe8, 0xdd, 0x74, 0x1f, 0x4b, 0xbd, 0x8b, 0x8a,\n",
        "    0x70, 0x3e, 0xb5, 0x66, 0x48, 0x03, 0xf6, 0x0e, 0x61, 0x35, 0x57, 0xb9, 0x86, 0xc1, 0x1d, 0x9e,\n",
        "    0xe1, 0xf8, 0x98, 0x11, 0x69, 0xd9, 0x8e, 0x94, 0x9b, 0x1e, 0x87, 0xe9, 0xce, 0x55, 0x28, 0xdf,\n",
        "    0x8c, 0xa1, 0x89, 0x0d, 0xbf, 0xe6, 0x42, 0x68, 0x41, 0x99, 0x2d, 0x0f, 0xb0, 0x54, 0xbb, 0x16\n",
        "], dtype=np.uint8)\n",
        "\n",
        "print(f\"AES S-box shape: {AES_SBOX.shape}\")\n",
        "print(f\"AES S-box dtype: {AES_SBOX.dtype}\")\n",
        "print(f\"First 16 values: {AES_SBOX[:16]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Block 5: Label Generation Function\n",
        "\n",
        "Generate labels for all 256 key hypotheses on-the-fly during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test plaintexts: ['0x0', '0x1', '0xff']\n",
            "Test labels shape: torch.Size([3, 256])\n",
            "Labels for first plaintext (0x00), first 8 key hypotheses: [1, 0, 1, 1, 0, 1, 1, 1]\n",
            "Labels for first plaintext (0x00), key hypothesis 0: 1\n",
            "Expected: LSB(Sbox[0x00 XOR 0x00]) = LSB(Sbox[0x00]) = LSB(0x63) = 1\n",
            "Got: 1\n"
          ]
        }
      ],
      "source": [
        "def get_all_labels(plaintexts, sbox):\n",
        "    \"\"\"\n",
        "    Generate labels for all 256 key hypotheses.\n",
        "    \n",
        "    For each key hypothesis k (0-255), compute:\n",
        "    label = LSB(Sbox[plaintext XOR k])\n",
        "    \n",
        "    Args:\n",
        "        plaintexts: (batch_size,) tensor of uint8 plaintext bytes\n",
        "        sbox: (256,) numpy array containing AES S-box lookup table\n",
        "    \n",
        "    Returns:\n",
        "        labels: (batch_size, 256) tensor of labels (0 or 1) - LSB of Sbox(plaintext XOR k)\n",
        "    \"\"\"\n",
        "    batch_size = plaintexts.shape[0]\n",
        "    labels = torch.zeros(batch_size, 256, dtype=torch.long, device=plaintexts.device)\n",
        "    \n",
        "    # Convert plaintexts to numpy for efficient indexing\n",
        "    plaintexts_np = plaintexts.cpu().numpy() if isinstance(plaintexts, torch.Tensor) else plaintexts\n",
        "    \n",
        "    # For each key hypothesis k\n",
        "    for k in range(256):\n",
        "        # Compute: intermediate = Sbox[plaintext XOR k]\n",
        "        intermediate = sbox[plaintexts_np ^ k]\n",
        "        # Extract LSB: label = intermediate & 1\n",
        "        labels[:, k] = torch.tensor(intermediate & 1, dtype=torch.long, device=plaintexts.device)\n",
        "    \n",
        "    return labels\n",
        "\n",
        "# Test label generation\n",
        "if DATASET_PATH.exists():\n",
        "    try:\n",
        "        test_plaintexts = torch.tensor([0x00, 0x01, 0xFF], dtype=torch.long)\n",
        "        test_labels = get_all_labels(test_plaintexts, AES_SBOX)\n",
        "        print(f\"Test plaintexts: {[hex(p.item()) for p in test_plaintexts]}\")\n",
        "        print(f\"Test labels shape: {test_labels.shape}\")\n",
        "        print(f\"Labels for first plaintext (0x00), first 8 key hypotheses: {test_labels[0, :8].tolist()}\")\n",
        "        print(f\"Labels for first plaintext (0x00), key hypothesis 0: {test_labels[0, 0].item()}\")\n",
        "        print(f\"Expected: LSB(Sbox[0x00 XOR 0x00]) = LSB(Sbox[0x00]) = LSB(0x63) = 1\")\n",
        "        print(f\"Got: {test_labels[0, 0].item()}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error testing label generation: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Block 6: Multi-Output MLP Model Architecture\n",
        "\n",
        "Implement the model with a shared layer and 256 independent branches.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model created on device: cuda\n",
            "Total parameters: 1,228,712\n",
            "Trainable parameters: 1,228,712\n",
            "\n",
            "Test input shape: torch.Size([1000, 700])\n",
            "Test output shape: torch.Size([1000, 256, 2])\n",
            "Expected output shape: (1000, 256, 2)\n",
            "✓ Model forward pass test passed!\n"
          ]
        }
      ],
      "source": [
        "class MultiOutputMLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Output MLP for Side-Channel Analysis.\n",
        "    \n",
        "    Architecture:\n",
        "    - Shared Layer: Linear(700, 200) → ReLU\n",
        "    - 256 Branches: Each branch takes shared output and applies:\n",
        "      Linear(200, 20) → ReLU → Linear(20, 10) → ReLU → Linear(10, 2)\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=700, shared_hidden=200, branch_hidden1=20, branch_hidden2=10, num_branches=256):\n",
        "        super(MultiOutputMLP, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.num_branches = num_branches\n",
        "        \n",
        "        # Shared layer: processes input traces\n",
        "        self.shared_layer = nn.Sequential(\n",
        "            nn.Linear(input_dim, shared_hidden),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        # 256 independent branches (one for each key hypothesis)\n",
        "        self.branches = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(shared_hidden, branch_hidden1),  # Branch hidden layer 1\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(branch_hidden1, branch_hidden2),  # Branch hidden layer 2\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(branch_hidden2, 2)  # Output 2 classes (LSB 0 or 1)\n",
        "            ) for _ in range(num_branches)\n",
        "        ])\n",
        "        \n",
        "        # Initialize weights using He uniform initialization\n",
        "        self._initialize_weights()\n",
        "    \n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"Initialize weights using He uniform initialization.\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, 700)\n",
        "        \n",
        "        Returns:\n",
        "            output: Tensor of shape (batch_size, 256, 2)\n",
        "        \"\"\"\n",
        "        batch_size = x.shape[0]\n",
        "        \n",
        "        # Shared layer: (batch_size, 700) → (batch_size, 200)\n",
        "        shared_output = self.shared_layer(x)\n",
        "        \n",
        "        # Process through each branch and stack outputs\n",
        "        branch_outputs = []\n",
        "        for branch in self.branches:\n",
        "            # Each branch: (batch_size, 200) → (batch_size, 2)\n",
        "            branch_out = branch(shared_output)\n",
        "            branch_outputs.append(branch_out)\n",
        "        \n",
        "        # Stack: (batch_size, 256, 2)\n",
        "        output = torch.stack(branch_outputs, dim=1)\n",
        "        \n",
        "        return output\n",
        "\n",
        "# Test model\n",
        "model = MultiOutputMLP(input_dim=TRACE_LENGTH, num_branches=NUM_BRANCHES).to(device)\n",
        "print(f\"Model created on device: {device}\")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# Test forward pass\n",
        "if DATASET_PATH.exists():\n",
        "    try:\n",
        "        test_input = torch.randn(BATCH_SIZE, TRACE_LENGTH).to(device)\n",
        "        test_output = model(test_input)\n",
        "        print(f\"\\nTest input shape: {test_input.shape}\")\n",
        "        print(f\"Test output shape: {test_output.shape}\")\n",
        "        print(f\"Expected output shape: ({BATCH_SIZE}, {NUM_BRANCHES}, 2)\")\n",
        "        assert test_output.shape == (BATCH_SIZE, NUM_BRANCHES, 2), f\"Output shape mismatch: {test_output.shape}\"\n",
        "        print(\"✓ Model forward pass test passed!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error testing model: {e}\")\n",
        "else:\n",
        "    print(\"Dataset not found - skipping forward pass test\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Block 7: Training Loop\n",
        "\n",
        "Train the model with multi-loss computation across all 256 branches.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training function defined. Ready to train!\n"
          ]
        }
      ],
      "source": [
        "def train_model(model, dataloader, num_epochs, device, correct_key=None):\n",
        "    \"\"\"\n",
        "    Training loop for Multi-Output MLP.\n",
        "    \n",
        "    Args:\n",
        "        model: MultiOutputMLP model\n",
        "        dataloader: DataLoader for training data\n",
        "        num_epochs: Number of training epochs\n",
        "        device: torch device\n",
        "        correct_key: Correct key byte value (0-255) for accuracy tracking. If None, will try to load from dataset.\n",
        "    \n",
        "    Returns:\n",
        "        correct_key_acc_history: List of correct key accuracies per epoch\n",
        "        wrong_key_acc_history: List of best wrong key accuracies per epoch\n",
        "    \"\"\"\n",
        "    # Initialize optimizer and loss function\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # Try to get correct key from dataset if not provided\n",
        "    if correct_key is None and DATASET_PATH.exists():\n",
        "        try:\n",
        "            with h5py.File(DATASET_PATH, 'r') as f:\n",
        "                if 'metadata' in f and 'key' in f['metadata']:\n",
        "                    key_array = f['metadata']['key'][:]\n",
        "                    # Assuming we're attacking the first key byte\n",
        "                    correct_key = int(key_array[0]) if len(key_array) > 0 else None\n",
        "                    print(f\"Found correct key in dataset: {key_array}\")\n",
        "                    print(f\"Using first byte as correct_key: {correct_key}\")\n",
        "                else:\n",
        "                    print(\"Warning: Could not find correct key in dataset. Accuracy tracking will be limited.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not load correct key from dataset: {e}\")\n",
        "    \n",
        "    # History for plotting\n",
        "    correct_key_acc_history = []\n",
        "    wrong_key_acc_history = []\n",
        "    loss_history = []\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0.0\n",
        "        epoch_correct_key_correct = 0\n",
        "        epoch_correct_key_total = 0\n",
        "        epoch_wrong_key_correct = 0\n",
        "        epoch_wrong_key_total = 0\n",
        "        \n",
        "        num_batches = 0\n",
        "        \n",
        "        for batch_idx, (traces, plaintexts) in enumerate(dataloader):\n",
        "            # Move to device\n",
        "            traces = traces.to(device)  # (batch_size, 700)\n",
        "            plaintexts = plaintexts.to(device)  # (batch_size,)\n",
        "            \n",
        "            # Get actual batch size (may vary on last batch)\n",
        "            actual_batch_size = traces.shape[0]\n",
        "            \n",
        "            # Forward pass\n",
        "            output = model(traces)  # (batch_size, 256, 2)\n",
        "            \n",
        "            # Generate labels for all 256 key hypotheses\n",
        "            labels = get_all_labels(plaintexts, AES_SBOX)  # (batch_size, 256)\n",
        "            \n",
        "            # Compute loss: sum of CrossEntropyLoss across all 256 branches\n",
        "            total_loss = 0\n",
        "            for k in range(256):\n",
        "                branch_output = output[:, k, :]  # (batch_size, 2)\n",
        "                branch_labels = labels[:, k]  # (batch_size,)\n",
        "                total_loss += criterion(branch_output, branch_labels)\n",
        "            \n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Track loss\n",
        "            epoch_loss += total_loss.item()\n",
        "            \n",
        "            # Calculate accuracy\n",
        "            # Get predictions: argmax over the 2 classes for each branch\n",
        "            preds = output.argmax(dim=2)  # (batch_size, 256)\n",
        "            \n",
        "            # Calculate accuracy for correct key (if known)\n",
        "            if correct_key is not None:\n",
        "                correct_key_preds = preds[:, correct_key]  # (batch_size,)\n",
        "                correct_key_labels = labels[:, correct_key]  # (batch_size,)\n",
        "                correct_key_correct = (correct_key_preds == correct_key_labels).sum().item()\n",
        "                epoch_correct_key_correct += correct_key_correct\n",
        "                epoch_correct_key_total += actual_batch_size\n",
        "            \n",
        "            # Calculate accuracy for wrong keys (all keys except correct one)\n",
        "            if correct_key is not None:\n",
        "                wrong_key_mask = torch.ones(256, dtype=torch.bool, device=device)\n",
        "                wrong_key_mask[correct_key] = False\n",
        "                wrong_key_preds = preds[:, wrong_key_mask]  # (batch_size, 255)\n",
        "                wrong_key_labels = labels[:, wrong_key_mask]  # (batch_size, 255)\n",
        "                wrong_key_correct = (wrong_key_preds == wrong_key_labels).sum().item()\n",
        "                epoch_wrong_key_correct += wrong_key_correct\n",
        "                epoch_wrong_key_total += wrong_key_preds.numel()\n",
        "            else:\n",
        "                # If correct key unknown, track all keys\n",
        "                all_correct = (preds == labels).sum().item()\n",
        "                epoch_wrong_key_correct += all_correct\n",
        "                epoch_wrong_key_total += preds.numel()\n",
        "            \n",
        "            num_batches += 1\n",
        "            \n",
        "            # Print progress every 10 batches\n",
        "            if (batch_idx + 1) % 10 == 0:\n",
        "                batch_loss = total_loss.item()\n",
        "                if correct_key is not None:\n",
        "                    batch_correct_acc = correct_key_correct / actual_batch_size\n",
        "                    batch_wrong_acc = wrong_key_correct / (255 * actual_batch_size)\n",
        "                    print(f\"  Batch {batch_idx + 1}/{len(dataloader)}: Loss={batch_loss:.4f}, \"\n",
        "                          f\"Correct Key Acc={batch_correct_acc:.4f}, Wrong Key Acc={batch_wrong_acc:.4f}\")\n",
        "        \n",
        "        # Calculate epoch averages\n",
        "        avg_loss = epoch_loss / num_batches\n",
        "        loss_history.append(avg_loss)\n",
        "        \n",
        "        if correct_key is not None:\n",
        "            correct_key_acc = epoch_correct_key_correct / epoch_correct_key_total if epoch_correct_key_total > 0 else 0.0\n",
        "            wrong_key_acc = epoch_wrong_key_correct / epoch_wrong_key_total if epoch_wrong_key_total > 0 else 0.0\n",
        "            correct_key_acc_history.append(correct_key_acc)\n",
        "            wrong_key_acc_history.append(wrong_key_acc)\n",
        "            \n",
        "            print(f\"Epoch {epoch + 1}/{num_epochs}: \"\n",
        "                  f\"Loss={avg_loss:.4f}, \"\n",
        "                  f\"Correct Key Acc={correct_key_acc:.4f}, \"\n",
        "                  f\"Wrong Key Acc={wrong_key_acc:.4f}\")\n",
        "        else:\n",
        "            all_acc = epoch_wrong_key_correct / epoch_wrong_key_total if epoch_wrong_key_total > 0 else 0.0\n",
        "            wrong_key_acc_history.append(all_acc)\n",
        "            print(f\"Epoch {epoch + 1}/{num_epochs}: Loss={avg_loss:.4f}, All Key Acc={all_acc:.4f}\")\n",
        "    \n",
        "    return correct_key_acc_history, wrong_key_acc_history, loss_history\n",
        "\n",
        "print(\"Training function defined. Ready to train!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "\"Could not find traces. Available keys: ['Attack_traces', 'Profiling_traces']\"",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create dataset and dataloader\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DATASET_PATH\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m----> 6\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mASCADDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATASET_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Get correct key for tracking (optional)\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[3], line 26\u001b[0m, in \u001b[0;36mASCADDataset.__init__\u001b[0;34m(self, h5_path, trace_key, plaintext_key)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraces\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find traces. Available keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(f\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Get plaintext key\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m plaintext_key \u001b[38;5;129;01min\u001b[39;00m f:\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Could not find traces. Available keys: ['Attack_traces', 'Profiling_traces']\""
          ]
        }
      ],
      "source": [
        "# Initialize model\n",
        "model = MultiOutputMLP(input_dim=TRACE_LENGTH, num_branches=NUM_BRANCHES).to(device)\n",
        "\n",
        "# Create dataset and dataloader\n",
        "if DATASET_PATH.exists():\n",
        "    dataset = ASCADDataset(DATASET_PATH)\n",
        "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "    \n",
        "    # Get correct key for tracking (optional)\n",
        "    correct_key = None\n",
        "    try:\n",
        "        with h5py.File(DATASET_PATH, 'r') as f:\n",
        "            if 'metadata' in f and 'key' in f['metadata']:\n",
        "                key_array = f['metadata']['key'][:]\n",
        "                correct_key = int(key_array[0]) if len(key_array) > 0 else None\n",
        "                print(f\"Correct key (first byte): {correct_key}\")\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    # Train model\n",
        "    print(\"Starting training...\")\n",
        "    correct_key_acc_history, wrong_key_acc_history, loss_history = train_model(\n",
        "        model, dataloader, NUM_EPOCHS, device, correct_key=correct_key\n",
        "    )\n",
        "    \n",
        "    print(\"\\nTraining completed!\")\n",
        "else:\n",
        "    print(\"Dataset file not found. Please add the HDF5 file to the dataset/ directory.\")\n",
        "    correct_key_acc_history = []\n",
        "    wrong_key_acc_history = []\n",
        "    loss_history = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Block 9: Visualization\n",
        "\n",
        "Plot the attack success rate: correct key accuracy vs wrong key accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No training history available. Please run training first.\n"
          ]
        }
      ],
      "source": [
        "# Plot training curves\n",
        "if len(correct_key_acc_history) > 0 and len(wrong_key_acc_history) > 0:\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    \n",
        "    # Plot 1: Accuracy comparison\n",
        "    plt.subplot(1, 2, 1)\n",
        "    epochs = range(1, len(correct_key_acc_history) + 1)\n",
        "    plt.plot(epochs, correct_key_acc_history, 'r-', label='Correct Key Accuracy', linewidth=2)\n",
        "    plt.plot(epochs, wrong_key_acc_history, 'b-', label='Wrong Key Accuracy', linewidth=2)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Attack Success Rate: Correct Key vs Wrong Keys')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.ylim([0.0, 1.0])\n",
        "    \n",
        "    # Add horizontal line at 0.5 (random guessing)\n",
        "    plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Random (50%)')\n",
        "    \n",
        "    # Plot 2: Loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, loss_history, 'g-', label='Training Loss', linewidth=2)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print final statistics\n",
        "    print(f\"\\nFinal Statistics:\")\n",
        "    print(f\"Correct Key Accuracy: {correct_key_acc_history[-1]:.4f} ({correct_key_acc_history[-1]*100:.2f}%)\")\n",
        "    print(f\"Wrong Key Accuracy: {wrong_key_acc_history[-1]:.4f} ({wrong_key_acc_history[-1]*100:.2f}%)\")\n",
        "    print(f\"Separation: {correct_key_acc_history[-1] - wrong_key_acc_history[-1]:.4f}\")\n",
        "    \n",
        "    if correct_key_acc_history[-1] > 0.63 and wrong_key_acc_history[-1] < 0.55:\n",
        "        print(\"✓ Attack successful! Correct key accuracy significantly higher than wrong keys.\")\n",
        "    elif correct_key_acc_history[-1] > wrong_key_acc_history[-1] + 0.1:\n",
        "        print(\"✓ Attack showing progress! Correct key accuracy is separating from wrong keys.\")\n",
        "    else:\n",
        "        print(\"⚠ Attack may need more training or hyperparameter tuning.\")\n",
        "else:\n",
        "    print(\"No training history available. Please run training first.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
