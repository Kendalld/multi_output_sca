1. Model Purpose and ContextThe $CNN_{MO}$ is designed specifically to reveal secret keys from de-synchronization countermeasures (where traces are randomly shifted)1111. Unlike the MLP variant, the CNN exploits translation-invariance to handle these shifts in a single training process2.2. Architecture OverviewThe model consists of an input layer, a shared layer composed of convolutional blocks, and a Multi-Output (MO) layer consisting of 256 branches3.Input LayerInput Size: 480 samples per trace (corresponding to the first Sbox output process)4444.Shared Layer StructureThe shared layer consists of two distinct blocks. The layers within each block are placed in the following specific order: Conv1D $\rightarrow$ Batch Normalization $\rightarrow$ Average Pooling $\rightarrow$ ReLU5.Block 1 Specifications:Convolution: 4 filters, kernel size $32 \times 1$6.Pooling: Average pooling size $2 \times 1$777.Normalization & Activation: Batch Normalization followed by ReLU8.Block 2 Specifications:Convolution: 4 filters, kernel size $16 \times 1$9.Pooling: Average pooling size $4 \times 1$10.Normalization & Activation: Batch Normalization followed by ReLU11.Multi-Output (Branch) LayerFollowing the shared layers, the network splits into $k$ branches where $k=256$ (one for each key hypothesis)12121212.Flattening: The output of the shared layer is flattened before entering the branches (implied by Fig 1(c))13.Hidden Layers per Branch: 0 (There are no hidden dense layers in the branches for the CNN model, unlike the MLP model)14.Output Layer per Branch: 2 neurons with Softmax activation (representing the binary classification of the LSB)15.3. Training HyperparametersBatch Size: 5016.Initialization: He_uniform17.Optimizer: Adaptive Moment Estimation (ADAM) with default settings18.4. Loss FunctionThe model uses a multi-loss approach where the total loss is the weighted sum of the losses from all 256 branches. The weights ($\gamma_{k}$) are equivalent (set to 1) for all branches191919.The total loss equation is:$$\mathcal{L}_{total}=\sum_{k=1}^{256}\gamma_{k}*\mathcal{L}^{[k]}(\theta)$$20Where $\mathcal{L}^{[k]}(\theta)$ is the loss for the $k$-th branch, calculated as:$$\mathcal{L}^{[k]}(\theta)=-\frac{1}{N_{s}}\sum_{j=1}^{2}y_{true}\ln(z)$$21$N_{s}$ denotes the number of training samples.The summation $j=1$ to $2$ refers to the binary classification classes (LSB 0 or 1).$y_{true}$ is the ground truth and $z$ is the predicted value.5. Dataset LabelingTo train this model, you must prepare the data such that it has 256 labels. The labeling method is the Least Significant Bit (LSB) of the Sbox output:$$l_{j}^{i}=LSB(Sbox(p_{i}\oplus k_{j}))$$22Where $p_i$ is the plaintext and $k_j$ is the key guess ($0$ to $255$)23.